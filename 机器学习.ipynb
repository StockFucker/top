{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from utils import *\n",
    "%pylab inline --no-import-all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.io #Used to load the OCTAVE *.mat files\n",
    "import scipy.misc #Used to show matrix as an image\n",
    "import matplotlib.cm as cm #Used to display images in a specific colormap\n",
    "import random #To pick random images to display\n",
    "import scipy.optimize #fmin_cg to train neural network\n",
    "import itertools\n",
    "from scipy.special import expit #Vectorized sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = data_df[(data_df.date > \"2015-01-01\") & (data_df.date < \"2016-01-01\")]\n",
    "data_df = data_df[data_df.date < \"2015-01-01\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_df = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(index=data_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = [\"yz0\",\"yz1\",\"minute\",\"minute2\",\"isnew\",\"opentop0\",\"opentop1\",\"opentop2\",\"reachtop1\",\"top0\",\"top1\",\"top2\",\"foot0\",\"foot1\",\"foot2\",\"jump1\",\"jump2\",\"speedup1\",\"speedup2\",\"small_volume\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for column in columns:\n",
    "    df[column] = eval(column).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df[\"volume_ratio\"] = data_df[\"minute_volume\"] / data_df[\"volume1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "change_se = data_df[\"change\"]\n",
    "change_se[change_se > 1.0] = 1\n",
    "change_se[change_se < 1.0] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'y' shape: (12778,). Unique elements in y: [ 0.  1.]\n",
      "'X' shape: (12778, 21). X[0] shape: (21,)\n"
     ]
    }
   ],
   "source": [
    "X = df.as_matrix()\n",
    "y = change_se.as_matrix()\n",
    "#Insert a column of 1's to X as usual\n",
    "X = np.insert(X,0,1,axis=1)\n",
    "print \"'y' shape: %s. Unique elements in y: %s\"%(y.shape,np.unique(y))\n",
    "print \"'X' shape: %s. X[0] shape: %s\"%(X.shape,X[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are some global variables I'm suing to ensure the sizes\n",
    "# of various matrices are correct\n",
    "#these are NOT including bias nits\n",
    "input_layer_size = 20\n",
    "hidden_layer_size = 7\n",
    "output_layer_size = 2 \n",
    "n_training_samples = X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Some utility functions. There are lot of flattening and\n",
    "#reshaping of theta matrices, the input X matrix, etc...\n",
    "#Nicely shaped matrices make the linear algebra easier when developing,\n",
    "#but the minimization routine (fmin_cg) requires that all inputs\n",
    "\n",
    "def flattenParams(thetas_list):\n",
    "    \"\"\"\n",
    "    Hand this function a list of theta matrices, and it will flatten it\n",
    "    into one long (n,1) shaped numpy array\n",
    "    \"\"\"\n",
    "    flattened_list = [ mytheta.flatten() for mytheta in thetas_list ]\n",
    "    combined = list(itertools.chain.from_iterable(flattened_list))\n",
    "    assert len(combined) == (input_layer_size+1)*hidden_layer_size + \\\n",
    "                            (hidden_layer_size+1)*output_layer_size\n",
    "    return np.array(combined).reshape((len(combined),1))\n",
    "\n",
    "def reshapeParams(flattened_array):\n",
    "    theta1 = flattened_array[:(input_layer_size+1)*hidden_layer_size] \\\n",
    "            .reshape((hidden_layer_size,input_layer_size+1))\n",
    "    theta2 = flattened_array[(input_layer_size+1)*hidden_layer_size:] \\\n",
    "            .reshape((output_layer_size,hidden_layer_size+1))\n",
    "    \n",
    "    return [ theta1, theta2 ]\n",
    "\n",
    "def flattenX(myX):\n",
    "    return np.array(myX.flatten()).reshape((n_training_samples*(input_layer_size+1),1))\n",
    "\n",
    "def reshapeX(flattenedX):\n",
    "    return np.array(flattenedX).reshape((n_training_samples,input_layer_size+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeCost(mythetas_flattened,myX_flattened,myy,mylambda=0.):\n",
    "    \"\"\"\n",
    "    This function takes in:\n",
    "        1) a flattened vector of theta parameters (each theta would go from one\n",
    "           NN layer to the next), the thetas include the bias unit.\n",
    "        2) the flattened training set matrix X, which contains the bias unit first column\n",
    "        3) the label vector y, which has one column\n",
    "    It loops over training points (recommended by the professor, as the linear\n",
    "    algebra version is \"quite complicated\") and:\n",
    "        1) constructs a new \"y\" vector, with 10 rows and 1 column, \n",
    "            with one non-zero entry corresponding to that iteration\n",
    "        2) computes the cost given that y- vector and that training point\n",
    "        3) accumulates all of the costs\n",
    "        4) computes a regularization term (after the loop over training points)\n",
    "    \"\"\"\n",
    "    \n",
    "    # First unroll the parameters\n",
    "    mythetas = reshapeParams(mythetas_flattened)\n",
    "    \n",
    "    # Now unroll X\n",
    "    myX = reshapeX(myX_flattened)\n",
    "    \n",
    "    #This is what will accumulate the total cost\n",
    "    total_cost = 0.\n",
    "    \n",
    "    m = n_training_samples\n",
    "\n",
    "    # Loop over the training points (rows in myX, already contain bias unit)\n",
    "    for irow in xrange(m):\n",
    "        myrow = myX[irow]\n",
    "                \n",
    "        # First compute the hypothesis (this is a (10,1) vector\n",
    "        # of the hypothesis for each possible y-value)\n",
    "        # propagateForward returns (zs, activations) for each layer\n",
    "        # so propagateforward[-1][1] means \"activation for -1st (last) layer\"\n",
    "        myhs = propagateForward(myrow,mythetas)[-1][1]\n",
    "\n",
    "        # Construct a 10x1 \"y\" vector with all zeros and only one \"1\" entry\n",
    "        # note here if the hand-written digit is \"0\", then that corresponds\n",
    "        # to a y- vector with 1 in the 10th spot (different from what the\n",
    "        # homework suggests)\n",
    "        tmpy  = np.zeros((output_layer_size,1))\n",
    "        tmpy[int(myy[irow]-1)] = 1\n",
    "        \n",
    "        # Compute the cost for this point and y-vector\n",
    "        mycost = -tmpy.T.dot(np.log(myhs))-(1-tmpy.T).dot(np.log(1-myhs))\n",
    "     \n",
    "        # Accumulate the total cost\n",
    "        total_cost += mycost\n",
    "  \n",
    "    # Normalize the total_cost, cast as float\n",
    "    total_cost = float(total_cost) / m\n",
    "    \n",
    "    # Compute the regularization term\n",
    "    total_reg = 0.\n",
    "    for mytheta in mythetas:\n",
    "        total_reg += np.sum(mytheta*mytheta) #element-wise multiplication\n",
    "    total_reg *= float(mylambda)/(2*m)\n",
    "        \n",
    "    return total_cost + total_reg\n",
    "       \n",
    "\n",
    "def propagateForward(row,Thetas):\n",
    "    \"\"\"\n",
    "    Function that given a list of Thetas (NOT flattened), propagates the\n",
    "    row of features forwards, assuming the features ALREADY\n",
    "    include the bias unit in the input layer, and the \n",
    "    Thetas also include the bias unit\n",
    "\n",
    "    The output is a vector with element [0] for the hidden layer,\n",
    "    and element [1] for the output layer\n",
    "        -- Each element is a tuple of (zs, as)\n",
    "        -- where \"zs\" and \"as\" have shape (# of units in that layer, 1)\n",
    "    \n",
    "    ***The 'activations' are the same as \"h\", but this works for many layers\n",
    "    (hence a vector of thetas, not just one theta)\n",
    "    Also, \"h\" is vectorized to do all rows at once...\n",
    "    this function takes in one row at a time***\n",
    "    \"\"\"\n",
    "   \n",
    "    features = row\n",
    "    zs_as_per_layer = []\n",
    "    for i in xrange(len(Thetas)):  \n",
    "        Theta = Thetas[i]\n",
    "        #Theta is (25,401), features are (401, 1)\n",
    "        #so \"z\" comes out to be (25, 1)\n",
    "        #this is one \"z\" value for each unit in the hidden layer\n",
    "        #not counting the bias unit\n",
    "        z = Theta.dot(features).reshape((Theta.shape[0],1))\n",
    "        a = expit(z)\n",
    "        zs_as_per_layer.append( (z, a) )\n",
    "        if i == len(Thetas)-1:\n",
    "            return np.array(zs_as_per_layer)\n",
    "        a = np.insert(a,0,1) #Add the bias unit\n",
    "        features = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genRandThetas():\n",
    "    epsilon_init = 0.12\n",
    "    theta1_shape = (hidden_layer_size, input_layer_size+1)\n",
    "    theta2_shape = (output_layer_size, hidden_layer_size+1)\n",
    "    rand_thetas = [ np.random.rand( *theta1_shape ) * 2 * epsilon_init - epsilon_init, \\\n",
    "                    np.random.rand( *theta2_shape ) * 2 * epsilon_init - epsilon_init]\n",
    "    return rand_thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoidGradient(z):\n",
    "    dummy = expit(z)\n",
    "    return dummy*(1-dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backPropagate(mythetas_flattened,myX_flattened,myy,mylambda=0.):\n",
    "    \n",
    "    # First unroll the parameters\n",
    "    mythetas = reshapeParams(mythetas_flattened)\n",
    "    \n",
    "    # Now unroll X\n",
    "    myX = reshapeX(myX_flattened)\n",
    "\n",
    "    #Note: the Delta matrices should include the bias unit\n",
    "    #The Delta matrices have the same shape as the theta matrices\n",
    "    Delta1 = np.zeros((hidden_layer_size,input_layer_size+1))\n",
    "    Delta2 = np.zeros((output_layer_size,hidden_layer_size+1))\n",
    "\n",
    "    # Loop over the training points (rows in myX, already contain bias unit)\n",
    "    m = n_training_samples\n",
    "    for irow in xrange(m):\n",
    "        myrow = myX[irow]\n",
    "        a1 = myrow.reshape((input_layer_size+1,1))\n",
    "        # propagateForward returns (zs, activations) for each layer excluding the input layer\n",
    "        temp = propagateForward(myrow,mythetas)\n",
    "        z2 = temp[0][0]\n",
    "        a2 = temp[0][1]\n",
    "        z3 = temp[1][0]\n",
    "        a3 = temp[1][1]\n",
    "        tmpy = np.zeros((output_layer_size,1))\n",
    "        tmpy[int(myy[irow]-1)] = 1\n",
    "        delta3 = a3 - tmpy \n",
    "        delta2 = mythetas[1].T[1:,:].dot(delta3)*sigmoidGradient(z2) #remove 0th element\n",
    "        a2 = np.insert(a2,0,1,axis=0)\n",
    "        Delta1 += delta2.dot(a1.T) #(25,1)x(1,401) = (25,401) (correct)\n",
    "        Delta2 += delta3.dot(a2.T) #(10,1)x(1,25) = (10,25) (should be 10,26)\n",
    "        \n",
    "    D1 = Delta1/float(m)\n",
    "    D2 = Delta2/float(m)\n",
    "    \n",
    "    #Regularization:\n",
    "    D1[:,1:] = D1[:,1:] + (float(mylambda)/m)*mythetas[0][:,1:]\n",
    "    D2[:,1:] = D2[:,1:] + (float(mylambda)/m)*mythetas[1][:,1:]\n",
    "    \n",
    "    return flattenParams([D1, D2]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3991318623\n"
     ]
    }
   ],
   "source": [
    "#Once you are done, using the loaded set of parameters Theta1 and Theta2,\n",
    "#and lambda = 1, you should see that the cost is about 0.383770\n",
    "myThetas = genRandThetas()\n",
    "print computeCost(flattenParams(myThetas),flattenX(X),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Actually compute D matrices for the Thetas provided\n",
    "flattenedD1D2 = backPropagate(flattenParams(myThetas),flattenX(X),y,mylambda=0.1)\n",
    "D1, D2 = reshapeParams(flattenedD1D2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkGradient(mythetas,myDs,myX,myy,mylambda=0.):\n",
    "    myeps = 0.0001\n",
    "    flattened = flattenParams(mythetas) #把backprop计算出来的theta拍平\n",
    "    flattenedDs = flattenParams(myDs) #把backprop计算出来的D拍平\n",
    "    myX_flattened = flattenX(myX)\n",
    "    n_elems = len(flattened) \n",
    "    #Pick ten random elements, compute numerical gradient, compare to respective D's\n",
    "    for i in xrange(10):\n",
    "        x = int(np.random.rand()*n_elems)\n",
    "        epsvec = np.zeros((n_elems,1)) #创建一个theta的数组\n",
    "        epsvec[x] = myeps #epsilon\n",
    "        cost_high = computeCost(flattened + epsvec,myX_flattened,myy,mylambda) #theta+ = flattened + epsvec\n",
    "        cost_low  = computeCost(flattened - epsvec,myX_flattened,myy,mylambda)  #theta+ = flattened - epsvec\n",
    "        mygrad = (cost_high - cost_low) / float(2*myeps)\n",
    "        print \"Element: %d. Numerical Gradient = %f. BackProp Gradient = %f.\"%(x,mygrad,flattenedDs[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element: 34. Numerical Gradient = 0.000371. BackProp Gradient = 0.000371.\n",
      "Element: 133. Numerical Gradient = 0.000588. BackProp Gradient = 0.000587.\n",
      "Element: 116. Numerical Gradient = -0.001009. BackProp Gradient = -0.001009.\n",
      "Element: 102. Numerical Gradient = 0.001330. BackProp Gradient = 0.001330.\n",
      "Element: 79. Numerical Gradient = 0.000242. BackProp Gradient = 0.000242.\n",
      "Element: 73. Numerical Gradient = 0.000161. BackProp Gradient = 0.000160.\n",
      "Element: 115. Numerical Gradient = -0.000093. BackProp Gradient = -0.000093.\n",
      "Element: 58. Numerical Gradient = 0.000404. BackProp Gradient = 0.000404.\n",
      "Element: 92. Numerical Gradient = 0.000078. BackProp Gradient = 0.000078.\n",
      "Element: 41. Numerical Gradient = 0.000392. BackProp Gradient = 0.000391.\n"
     ]
    }
   ],
   "source": [
    "checkGradient(myThetas,[D1, D2],X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Here I will use scipy.optimize.fmin_cg\n",
    "\n",
    "def trainNN(mylambda=0.):\n",
    "    \"\"\"\n",
    "    Function that generates random initial theta matrices, optimizes them,\n",
    "    and returns a list of two re-shaped theta matrices\n",
    "    \"\"\"\n",
    "\n",
    "    randomThetas_unrolled = flattenParams(genRandThetas())\n",
    "    result = scipy.optimize.fmin_cg(computeCost, x0=randomThetas_unrolled, fprime=backPropagate, \\\n",
    "                               args=(flattenX(X),y,mylambda),maxiter=1000,disp=True,full_output=True)\n",
    "    return reshapeParams(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.795612\n",
      "         Iterations: 1000\n",
      "         Function evaluations: 1921\n",
      "         Gradient evaluations: 1920\n"
     ]
    }
   ],
   "source": [
    "#Training the NN takes about ~70-80 seconds on my machine\n",
    "learned_Thetas = trainNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictNN(row,Thetas):\n",
    "    \"\"\"\n",
    "    Function that takes a row of features, propagates them through the\n",
    "    NN, and returns the predicted integer that was hand written\n",
    "    \"\"\"\n",
    "    classes = range(0,2)\n",
    "    output = propagateForward(row,Thetas)\n",
    "    #-1 means last layer, 1 means \"a\" instead of \"z\"\n",
    "    return classes[np.argmax(output[-1][1])] \n",
    "\n",
    "def computeAccuracy(myX,myThetas,myy):\n",
    "    \"\"\"\n",
    "    Function that loops over all of the rows in X (all of the handwritten images)\n",
    "    and predicts what digit is written given the thetas. Check if it's correct, and\n",
    "    compute an efficiency.\n",
    "    \"\"\"\n",
    "    n_correct, n_total = 0, myX.shape[0]\n",
    "    for irow in xrange(n_total):\n",
    "#         print int(predictNN(myX[irow],myThetas))\n",
    "        if int(predictNN(myX[irow],myThetas)) != int(myy[irow]): \n",
    "            n_correct += 1\n",
    "    print \"Training set accuracy: %0.1f%%\"%(100*(float(n_correct)/n_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 81.3%\n"
     ]
    }
   ],
   "source": [
    "computeAccuracy(X,learned_Thetas,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's see if I set lambda to 10, if I get the same thing\n",
    "learned_regularized_Thetas = trainNN(mylambda=10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "computeAccuracy(X,learned_regularized_Thetas,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
