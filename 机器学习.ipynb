{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from utils import *\n",
    "%pylab inline --no-import-all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.io #Used to load the OCTAVE *.mat files\n",
    "import scipy.misc #Used to show matrix as an image\n",
    "import matplotlib.cm as cm #Used to display images in a specific colormap\n",
    "import random #To pick random images to display\n",
    "import scipy.optimize #fmin_cg to train neural network\n",
    "import itertools\n",
    "from scipy.special import expit #Vectorized sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_df = data_df[data_df.date < \"2015-01-01\"]\n",
    "data_df = data_df[(data_df.date > \"2015-01-01\") & (data_df.date < \"2016-01-01\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_df = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(index=data_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = [\"yz0\",\"yz1\",\"minute\",\"minute2\",\"isnew\",\"opentop0\",\"opentop1\",\"opentop2\",\"reachtop1\",\"top0\",\"top1\",\"top2\",\"foot0\",\"foot1\",\"foot2\",\"jump1\",\"jump2\",\"speedup1\",\"speedup2\",\"small_volume\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for column in columns:\n",
    "    df[column] = eval(column).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13185"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df[\"volume_ratio\"] = data_df[\"minute_volume\"] / data_df[\"volume1\"\n",
    "len(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "change_se = data_df[\"change\"].copy()\n",
    "change_se[change_se > 1.0] = 1\n",
    "change_se[change_se < 1.0] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'y' shape: (13185,). Unique elements in y: [ 0.  1.]\n",
      "'X' shape: (13185, 21). X[0] shape: (21,)\n"
     ]
    }
   ],
   "source": [
    "X = df.as_matrix()\n",
    "y = change_se.as_matrix()\n",
    "#Insert a column of 1's to X as usual\n",
    "X = np.insert(X,0,1,axis=1)\n",
    "print \"'y' shape: %s. Unique elements in y: %s\"%(y.shape,np.unique(y))\n",
    "print \"'X' shape: %s. X[0] shape: %s\"%(X.shape,X[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are some global variables I'm suing to ensure the sizes\n",
    "# of various matrices are correct\n",
    "#these are NOT including bias nits\n",
    "input_layer_size = 20\n",
    "hidden_layer_size = 7\n",
    "output_layer_size = 2 \n",
    "n_training_samples = X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Some utility functions. There are lot of flattening and\n",
    "#reshaping of theta matrices, the input X matrix, etc...\n",
    "#Nicely shaped matrices make the linear algebra easier when developing,\n",
    "#but the minimization routine (fmin_cg) requires that all inputs\n",
    "\n",
    "def flattenParams(thetas_list):\n",
    "    \"\"\"\n",
    "    Hand this function a list of theta matrices, and it will flatten it\n",
    "    into one long (n,1) shaped numpy array\n",
    "    \"\"\"\n",
    "    flattened_list = [ mytheta.flatten() for mytheta in thetas_list ]\n",
    "    combined = list(itertools.chain.from_iterable(flattened_list))\n",
    "    assert len(combined) == (input_layer_size+1)*hidden_layer_size + \\\n",
    "                            (hidden_layer_size+1)*output_layer_size\n",
    "    return np.array(combined).reshape((len(combined),1))\n",
    "\n",
    "def reshapeParams(flattened_array):\n",
    "    theta1 = flattened_array[:(input_layer_size+1)*hidden_layer_size] \\\n",
    "            .reshape((hidden_layer_size,input_layer_size+1))\n",
    "    theta2 = flattened_array[(input_layer_size+1)*hidden_layer_size:] \\\n",
    "            .reshape((output_layer_size,hidden_layer_size+1))\n",
    "    \n",
    "    return [ theta1, theta2 ]\n",
    "\n",
    "def flattenX(myX):\n",
    "    return np.array(myX.flatten()).reshape((n_training_samples*(input_layer_size+1),1))\n",
    "\n",
    "def reshapeX(flattenedX):\n",
    "    return np.array(flattenedX).reshape((n_training_samples,input_layer_size+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeCost(mythetas_flattened,myX_flattened,myy,mylambda=0.):\n",
    "    \"\"\"\n",
    "    This function takes in:\n",
    "        1) a flattened vector of theta parameters (each theta would go from one\n",
    "           NN layer to the next), the thetas include the bias unit.\n",
    "        2) the flattened training set matrix X, which contains the bias unit first column\n",
    "        3) the label vector y, which has one column\n",
    "    It loops over training points (recommended by the professor, as the linear\n",
    "    algebra version is \"quite complicated\") and:\n",
    "        1) constructs a new \"y\" vector, with 10 rows and 1 column, \n",
    "            with one non-zero entry corresponding to that iteration\n",
    "        2) computes the cost given that y- vector and that training point\n",
    "        3) accumulates all of the costs\n",
    "        4) computes a regularization term (after the loop over training points)\n",
    "    \"\"\"\n",
    "    \n",
    "    # First unroll the parameters\n",
    "    mythetas = reshapeParams(mythetas_flattened)\n",
    "    \n",
    "    # Now unroll X\n",
    "    myX = reshapeX(myX_flattened)\n",
    "    \n",
    "    #This is what will accumulate the total cost\n",
    "    total_cost = 0.\n",
    "    \n",
    "    m = n_training_samples\n",
    "\n",
    "    # Loop over the training points (rows in myX, already contain bias unit)\n",
    "    for irow in xrange(m):\n",
    "        myrow = myX[irow]\n",
    "                \n",
    "        # First compute the hypothesis (this is a (10,1) vector\n",
    "        # of the hypothesis for each possible y-value)\n",
    "        # propagateForward returns (zs, activations) for each layer\n",
    "        # so propagateforward[-1][1] means \"activation for -1st (last) layer\"\n",
    "        myhs = propagateForward(myrow,mythetas)[-1][1]\n",
    "\n",
    "        # Construct a 10x1 \"y\" vector with all zeros and only one \"1\" entry\n",
    "        # note here if the hand-written digit is \"0\", then that corresponds\n",
    "        # to a y- vector with 1 in the 10th spot (different from what the\n",
    "        # homework suggests)\n",
    "        tmpy  = np.zeros((output_layer_size,1))\n",
    "        tmpy[int(myy[irow]-1)] = 1\n",
    "        \n",
    "        # Compute the cost for this point and y-vector\n",
    "        mycost = -tmpy.T.dot(np.log(myhs))-(1-tmpy.T).dot(np.log(1-myhs))\n",
    "     \n",
    "        # Accumulate the total cost\n",
    "        total_cost += mycost\n",
    "  \n",
    "    # Normalize the total_cost, cast as float\n",
    "    total_cost = float(total_cost) / m\n",
    "    \n",
    "    # Compute the regularization term\n",
    "    total_reg = 0.\n",
    "    for mytheta in mythetas:\n",
    "        total_reg += np.sum(mytheta*mytheta) #element-wise multiplication\n",
    "    total_reg *= float(mylambda)/(2*m)\n",
    "        \n",
    "    return total_cost + total_reg\n",
    "       \n",
    "\n",
    "def propagateForward(row,Thetas):\n",
    "    \"\"\"\n",
    "    Function that given a list of Thetas (NOT flattened), propagates the\n",
    "    row of features forwards, assuming the features ALREADY\n",
    "    include the bias unit in the input layer, and the \n",
    "    Thetas also include the bias unit\n",
    "\n",
    "    The output is a vector with element [0] for the hidden layer,\n",
    "    and element [1] for the output layer\n",
    "        -- Each element is a tuple of (zs, as)\n",
    "        -- where \"zs\" and \"as\" have shape (# of units in that layer, 1)\n",
    "    \n",
    "    ***The 'activations' are the same as \"h\", but this works for many layers\n",
    "    (hence a vector of thetas, not just one theta)\n",
    "    Also, \"h\" is vectorized to do all rows at once...\n",
    "    this function takes in one row at a time***\n",
    "    \"\"\"\n",
    "   \n",
    "    features = row\n",
    "    zs_as_per_layer = []\n",
    "    for i in xrange(len(Thetas)):  \n",
    "        Theta = Thetas[i]\n",
    "        #Theta is (25,401), features are (401, 1)\n",
    "        #so \"z\" comes out to be (25, 1)\n",
    "        #this is one \"z\" value for each unit in the hidden layer\n",
    "        #not counting the bias unit\n",
    "        z = Theta.dot(features).reshape((Theta.shape[0],1))\n",
    "        a = expit(z)\n",
    "        zs_as_per_layer.append( (z, a) )\n",
    "        if i == len(Thetas)-1:\n",
    "            return np.array(zs_as_per_layer)\n",
    "        a = np.insert(a,0,1) #Add the bias unit\n",
    "        features = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genRandThetas():\n",
    "    epsilon_init = 0.12\n",
    "    theta1_shape = (hidden_layer_size, input_layer_size+1)\n",
    "    theta2_shape = (output_layer_size, hidden_layer_size+1)\n",
    "    rand_thetas = [ np.random.rand( *theta1_shape ) * 2 * epsilon_init - epsilon_init, \\\n",
    "                    np.random.rand( *theta2_shape ) * 2 * epsilon_init - epsilon_init]\n",
    "    return rand_thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoidGradient(z):\n",
    "    dummy = expit(z)\n",
    "    return dummy*(1-dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backPropagate(mythetas_flattened,myX_flattened,myy,mylambda=0.):\n",
    "    \n",
    "    # First unroll the parameters\n",
    "    mythetas = reshapeParams(mythetas_flattened)\n",
    "    \n",
    "    # Now unroll X\n",
    "    myX = reshapeX(myX_flattened)\n",
    "\n",
    "    #Note: the Delta matrices should include the bias unit\n",
    "    #The Delta matrices have the same shape as the theta matrices\n",
    "    Delta1 = np.zeros((hidden_layer_size,input_layer_size+1))\n",
    "    Delta2 = np.zeros((output_layer_size,hidden_layer_size+1))\n",
    "\n",
    "    # Loop over the training points (rows in myX, already contain bias unit)\n",
    "    m = n_training_samples\n",
    "    for irow in xrange(m):\n",
    "        myrow = myX[irow]\n",
    "        a1 = myrow.reshape((input_layer_size+1,1))\n",
    "        # propagateForward returns (zs, activations) for each layer excluding the input layer\n",
    "        temp = propagateForward(myrow,mythetas)\n",
    "        z2 = temp[0][0]\n",
    "        a2 = temp[0][1]\n",
    "        z3 = temp[1][0]\n",
    "        a3 = temp[1][1]\n",
    "        tmpy = np.zeros((output_layer_size,1))\n",
    "        tmpy[int(myy[irow]-1)] = 1\n",
    "        delta3 = a3 - tmpy \n",
    "        delta2 = mythetas[1].T[1:,:].dot(delta3)*sigmoidGradient(z2) #remove 0th element\n",
    "        a2 = np.insert(a2,0,1,axis=0)\n",
    "        Delta1 += delta2.dot(a1.T) #(25,1)x(1,401) = (25,401) (correct)\n",
    "        Delta2 += delta3.dot(a2.T) #(10,1)x(1,25) = (10,25) (should be 10,26)\n",
    "        \n",
    "    D1 = Delta1/float(m)\n",
    "    D2 = Delta2/float(m)\n",
    "    \n",
    "    #Regularization:\n",
    "    D1[:,1:] = D1[:,1:] + (float(mylambda)/m)*mythetas[0][:,1:]\n",
    "    D2[:,1:] = D2[:,1:] + (float(mylambda)/m)*mythetas[1][:,1:]\n",
    "    \n",
    "    return flattenParams([D1, D2]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.39804525273\n"
     ]
    }
   ],
   "source": [
    "#Once you are done, using the loaded set of parameters Theta1 and Theta2,\n",
    "#and lambda = 1, you should see that the cost is about 0.383770\n",
    "myThetas = genRandThetas()\n",
    "print computeCost(flattenParams(myThetas),flattenX(X),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Actually compute D matrices for the Thetas provided\n",
    "flattenedD1D2 = backPropagate(flattenParams(myThetas),flattenX(X),y,mylambda=0.1)\n",
    "D1, D2 = reshapeParams(flattenedD1D2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkGradient(mythetas,myDs,myX,myy,mylambda=0.):\n",
    "    myeps = 0.0001\n",
    "    flattened = flattenParams(mythetas) #把backprop计算出来的theta拍平\n",
    "    flattenedDs = flattenParams(myDs) #把backprop计算出来的D拍平\n",
    "    myX_flattened = flattenX(myX)\n",
    "    n_elems = len(flattened) \n",
    "    #Pick ten random elements, compute numerical gradient, compare to respective D's\n",
    "    for i in xrange(10):\n",
    "        x = int(np.random.rand()*n_elems)\n",
    "        epsvec = np.zeros((n_elems,1)) #创建一个theta的数组\n",
    "        epsvec[x] = myeps #epsilon\n",
    "        cost_high = computeCost(flattened + epsvec,myX_flattened,myy,mylambda) #theta+ = flattened + epsvec\n",
    "        cost_low  = computeCost(flattened - epsvec,myX_flattened,myy,mylambda)  #theta+ = flattened - epsvec\n",
    "        mygrad = (cost_high - cost_low) / float(2*myeps)\n",
    "        print \"Element: %d. Numerical Gradient = %f. BackProp Gradient = %f.\"%(x,mygrad,flattenedDs[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element: 71. Numerical Gradient = 0.000141. BackProp Gradient = 0.000142.\n",
      "Element: 94. Numerical Gradient = -0.000268. BackProp Gradient = -0.000268.\n",
      "Element: 146. Numerical Gradient = -0.002244. BackProp Gradient = -0.002244.\n",
      "Element: 54. Numerical Gradient = -0.005950. BackProp Gradient = -0.005950.\n",
      "Element: 6. Numerical Gradient = 0.000019. BackProp Gradient = 0.000020.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-0c672d04b8ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcheckGradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyThetas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mD1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-ba7c7eb08a4f>\u001b[0m in \u001b[0;36mcheckGradient\u001b[0;34m(mythetas, myDs, myX, myy, mylambda)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mepsvec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyeps\u001b[0m \u001b[0;31m#epsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mcost_high\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomputeCost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflattened\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mepsvec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmyX_flattened\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmyy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmylambda\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#theta+ = flattened + epsvec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mcost_low\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mcomputeCost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflattened\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mepsvec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmyX_flattened\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmyy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmylambda\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#theta+ = flattened - epsvec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mmygrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcost_high\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcost_low\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmyeps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Element: %d. Numerical Gradient = %f. BackProp Gradient = %f.\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmygrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mflattenedDs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-ddd3506f197b>\u001b[0m in \u001b[0;36mcomputeCost\u001b[0;34m(mythetas_flattened, myX_flattened, myy, mylambda)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# propagateForward returns (zs, activations) for each layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# so propagateforward[-1][1] means \"activation for -1st (last) layer\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mmyhs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpropagateForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyrow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmythetas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Construct a 10x1 \"y\" vector with all zeros and only one \"1\" entry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-ddd3506f197b>\u001b[0m in \u001b[0;36mpropagateForward\u001b[0;34m(row, Thetas)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mzs_as_per_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mThetas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzs_as_per_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Add the bias unit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "checkGradient(myThetas,[D1, D2],X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Here I will use scipy.optimize.fmin_cg\n",
    "\n",
    "def trainNN(mylambda=0.):\n",
    "    \"\"\"\n",
    "    Function that generates random initial theta matrices, optimizes them,\n",
    "    and returns a list of two re-shaped theta matrices\n",
    "    \"\"\"\n",
    "\n",
    "    randomThetas_unrolled = flattenParams(genRandThetas())\n",
    "    result = scipy.optimize.fmin_cg(computeCost, x0=randomThetas_unrolled, fprime=backPropagate, \\\n",
    "                               args=(flattenX(X),y,mylambda),maxiter=100,disp=True,full_output=True)\n",
    "    return reshapeParams(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.787061\n",
      "         Iterations: 100\n",
      "         Function evaluations: 215\n",
      "         Gradient evaluations: 215\n"
     ]
    }
   ],
   "source": [
    "#Training the NN takes about ~70-80 seconds on my machine\n",
    "learned_Thetas = trainNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictNN(row,Thetas):\n",
    "    \"\"\"\n",
    "    Function that takes a row of features, propagates them through the\n",
    "    NN, and returns the predicted integer that was hand written\n",
    "    \"\"\"\n",
    "    classes = range(0,2)\n",
    "    output = propagateForward(row,Thetas)\n",
    "    #-1 means last layer, 1 means \"a\" instead of \"z\"\n",
    "#     print classes,np.argmax(output[-1][1])\n",
    "    return classes[np.argmax(output[-1][1])] \n",
    "\n",
    "def computeAccuracy(myX,myThetas,myy):\n",
    "    \"\"\"\n",
    "    Function that loops over all of the rows in X (all of the handwritten images)\n",
    "    and predicts what digit is written given the thetas. Check if it's correct, and\n",
    "    compute an efficiency.\n",
    "    \"\"\"\n",
    "    n_correct, n_total = 0, myX.shape[0]\n",
    "    for irow in xrange(n_total):\n",
    "#         print int(predictNN(myX[irow],myThetas))\n",
    "        if int(predictNN(myX[irow],myThetas)) != int(myy[irow]): \n",
    "            n_correct += 1\n",
    "    print \"Training set accuracy: %0.1f%%\"%(100*(float(n_correct)/n_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 81.4%\n"
     ]
    }
   ],
   "source": [
    "computeAccuracy(X,learned_Thetas,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's see if I set lambda to 10, if I get the same thing\n",
    "learned_regularized_Thetas = trainNN(mylambda=10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "computeAccuracy(X,learned_regularized_Thetas,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.037694\n",
       "1    0.957447\n",
       "2    1.017380\n",
       "dtype: float64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_se = pd.Series(data_df[\"change\"].as_matrix())\n",
    "index_se = data_df.index\n",
    "y_se.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5681 7752 2.27131958385e+63\n",
      "Training set accuracy: 73.3%\n"
     ]
    }
   ],
   "source": [
    "n_correct, n_total = 0,0\n",
    "\n",
    "net_value = 1.0\n",
    "indexs = []\n",
    "for irow in xrange( X.shape[0]):\n",
    "#         print int(predictNN(myX[irow],myThetas))\n",
    "#     if int(predictNN(X[irow],learned_Thetas)) != int(y[irow]): \n",
    "#         n_correct += 1\n",
    "#     print int(predictNN(X[irow],learned_Thetas))\n",
    "\n",
    "    y_value = y_se[irow]\n",
    "    index = index_se[irow]\n",
    "    if int(predictNN(X[irow],learned_Thetas)) == 0:\n",
    "        n_total += 1\n",
    "        net_value = net_value * y_value\n",
    "        indexs.append(index)\n",
    "        if y_value >= 1.0:\n",
    "            n_correct += 1\n",
    "#     else:\n",
    "#         if y_value < 1.0:\n",
    "#             n_correct += 1\n",
    "print n_correct,n_total,net_value\n",
    "print \"Training set accuracy: %0.1f%%\"%(100*(float(n_correct)/n_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13185"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_df = data_df[data_df.index.isin(indexs) == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            mean  count\n",
      "recent                 \n",
      "1.0     1.020701    439\n",
      "2.0     1.030489    900\n",
      "3.0     1.032136    163\n",
      "4.0     1.043572     52\n",
      "5.0     1.036527     29\n",
      "6.0     1.038850     17\n",
      "7.0     0.989046     11\n",
      "8.0     1.042653      6\n",
      "9.0     1.012009      8\n",
      "10.0    1.012153      4\n",
      "12.0    1.033260      2\n",
      "14.0    1.053998      2\n",
      "0.697986577181 1639 1.02843349682\n"
     ]
    }
   ],
   "source": [
    "valid_df = valid_df.sort_values(\"minute\")\n",
    "valid_df = valid_df.groupby(\"date\").first()\n",
    "result(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
